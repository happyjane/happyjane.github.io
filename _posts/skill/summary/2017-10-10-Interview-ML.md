---
layout: post
title: "机器学习面试题"
date: 2017-09-27 18:00:00 +0800 
categories: 酱油人生
tag: Machine Learning
---
* content
{:toc}

#### 1.	给你一个有1000列和1百万行的训练数据集。这个数据集是基于分类问题的。经理要求你来降低该数据集的维度以减少模型计算时间。你的机器内存有限。你会怎么做？（你可以自由做各种实际操作假设。）
 1.由于我们的RAM很小，首先要关闭机器上正在运行的其他程序，包括网页浏览器，以确保大部分内存可以使用。
 2.我们可以随机采样数据集。这意味着，我们可以创建一个较小的数据集，比如有1000个变量和30万行，然后做计算。
 3.为了降低维度，我们可以把数值变量和分类变量分开，同时删掉相关联的变量。对于数值变量，我们将使用相关性分析。对于分类变量，我们可以用卡方检验。
 4.另外，我们还可以使用PCA（主成分分析），并挑选可以解释在数据集中有最大偏差的成分。
 5.利用在线学习算法，如VowpalWabbit（在Python中可用）是一个可能的选择。
 6.利用Stochastic GradientDescent（随机梯度下降）法建立线性模型也很有帮助。
 7.我们也可以用我们对业务的理解来估计各预测变量对响应变量的影响大小。但是，这是一个主观的方法，如果没有找出有用的预测变量可能会导致信息的显著丢失。
 注意：对于第4和第5点，请务必阅读有关在线学习算法和随机梯度下降法的内容。这些是高阶方法。
#### 2.	在PCA中有必要做旋转变换吗？如果有必要，为什么？如果你没有旋转变换那些成分，会发生什么情况？
 答：是的，旋转（正交）是必要的，因为它把由主成分捕获的方差之间的差异最大化。这使得主成分更容易解释。
 但是不要忘记我们做PCA的目的是选择更少的主成分（与特征变量个数相较而言），那些选上的主成分能够解释数据集中最大方差。通过做旋转，各主成分的相对位置不发生变化，它只能改变点的实际坐标。
如果我们没有旋转主成分，PCA的效果会减弱，那样我们会不得不选择更多个主成分来解释数据集里的方差。
 注意：对PCA（主成分分析）需要了解更多。
#### 3.	给你一个数据集。这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？
 答：这个问题给了你足够的提示来开始思考！由于数据分布在中位数附近，让我们先假设这是一个正态分布。我们知道，在一个正态分布中，约有68％的数据位于跟平均数（或众数、中位数）1个标准差范围内的，那样剩下的约32%的数据是不受影响的。因此，约有32%的数据将不受到缺失值的影响。

#### 4.	为什么朴素贝叶斯如此“朴素”？
 
答：朴素贝叶斯太‘朴素’了，因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的。
#### 5.	解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？
 
答：先验概率就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。例如，在一个数据集中，因变量是二进制的（1和0）。例如，1（垃圾邮件）的比例为70％和0（非垃圾邮件）的为30％。
 
因此，我们可以估算出任何新的电子邮件有70％的概率被归类为垃圾邮件。似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。边际似然估计就是，“FREE”这个词在任何消息中使用的概率。
 
#### 6.	你正在一个时间序列数据集上工作。经理要求你建立一个高精度的模型。你开始用决策树算法，因为你知道它在所有类型数据上的表现都不错。后来，你尝试了时间序列回归模型，并得到了比决策树模型更高的精度。这种情况会发生吗？为什么？
 
答：众所周知，时间序列数据有线性关系。另一方面，决策树算法是已知的检测非线性交互最好的算法。为什么决策树没能提供好的预测的原因是它不能像回归模型一样做到对线性关系的那么好的映射。因此，我们知道了如果我们有一个满足线性假设的数据集，一个线性回归模型能提供强大的预测。
#### 7.	KNN和KMEANS聚类（kmeans clustering）有什么不同？

答：不要被它们的名字里的“K”误导。你应该知道，这两种算法之间的根本区别是，KMEANS本质上是无监督学习而KNN是监督学习。KMEANS是聚类算法。KNN是分类（或回归）算法。 KMEAN算法把一个数据集分割成簇，使得形成的簇是同构的，每个簇里的点相互靠近。该算法试图维持这些簇之间有足够的可分离性。由于无监督的性质，这些簇没有任何标签。NN算法尝试基于其k（可以是任何数目）个周围邻居来对未标记的观察进行分类。它也被称为懒惰学习法，因为它涉及最小的模型训练。因此，它不用训练数据对未看见的数据集进行泛化。
#### 8.	什么时候Ridge回归优于Lasso回归？
 
答：你可以引用ISLR的作者Hastie和Tibshirani的话，他们断言在对少量变量有中等或大尺度的影响的时候用lasso回归。在对多个变量只有小或中等尺度影响的时候，使用Ridge回归。
 
从概念上讲，我们可以说，Lasso回归（L1）同时做变量选择和参数收缩，而ridge回归只做参数收缩，并最终在模型中包含所有的系数。在有相关变量时，ridge回归可能是首选。此外，ridge回归在用最小二乘估计有更高的偏差的情况下效果最好。因此，选择合适的模型取决于我们的模型的目标。
#### 9.	协方差和相关性有什么区别？
 答：相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。
#### 10.	Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？
 
答：最根本的区别是，随机森林算法使用bagging技术做出预测。 GBM采用boosting技术做预测。在bagging技术中，数据集用随机采样的方法被划分成使n个样本。然后，使用单一的学习算法，在所有样本上建模。接着利用投票或者求平均来组合所得到的预测。Bagging是平行进行的。而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在后续一轮中得到校正。
 
这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。在另一方面，GBM提高了精度，同时减少了模型的偏差和方差。
 
#### 11.	运行二元分类树算法很容易，但是你知道一个树是如何做分割的吗，即树如何决定把哪些变量分到哪个根节点和后续节点上？
 
答：分类树利用基尼系数与节点熵来做决定。简而言之，树算法找到最好的可能特征，它可以将数据集分成最纯的可能子节点。树算法找到可以把数据集分成最纯净的可能的子节点的特征量。基尼系数是，如果总体是完全纯的，那么我们从总体中随机选择2个样本，而这2个样本肯定是同一类的而且它们是同类的概率也是1。我们可以用以下方法计算基尼系数：
 
1.利用成功和失败的概率的平方和(p^2+q^2)计算子节点的基尼系数
2.利用该分割的节点的加权基尼分数计算基尼系数以分割
#### 12.你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？
 
答：该模型过度拟合。训练误差为0.00意味着分类器已在一定程度上模拟了训练数据，这样的分类器是不能用在未看见的数据上的。因此，当该分类器用于未看见的样本上时，由于找不到已有的模式，就会返回的预测有很高的错误率。
 
在随机森林算法中，用了多于需求个数的树时，这种情况会发生。因此，为了避免这些情况，我们要用交叉验证来调整树的数量。
#### 13.	给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？
答：我们可以用下面的方法来处理：
1.把缺失值分成单独的一类，这些缺失值说不定会包含一些趋势信息。
2.我们可以毫无顾忌地删除它们。
3.或者，我们可以用目标变量来检查它们的分布，如果发现任何模式，我们将保留那些缺失值并给它们一个新的分类，同时删除其他缺失值。
问：你有用过协同过滤算法吗？可以讲讲算法原理以及使用思路吗？
协同过滤算法考虑用于推荐项目的“用户行为”。它们利用的是其他用户的购买行为和针对商品的交易历史记录、评分、选择和购买信息。针对商品的其他用户的行为和偏好用来推荐项目（商品）给新用户。在这种情况下，项目（商品）的特征是未知的。
#### 答：你应该说，机器学习算法的选择完全取决于数据的类型。如果给定的一个数据集是线性的，线性回归是最好的选择。如果数据是图像或者音频，那么神经网络可以构建一个稳健的模型。如果该数据是非线性互相作用的的，可以用boosting或bagging算法。
#### 15.	什么时候正则化在机器学习中是有必要的？
答：当模型过度拟合或者欠拟合的时候，正则化是有必要的。这个技术引入了一个成本项，用于带来目标函数的更多特征。因此，正则化是将许多变量的系数推向零，由此而降低成本项。这有助于降低模型的复杂度，使该模型可以在预测上（泛化）变得更好。
问：你之前用过自然语言处理算法之类的吗？用过哪些，能举个应用实例吗？
问：之前有用过智能推荐算法吗？用过哪些，能举个实例吗？

#### 16.	为什么一些机器学习模型需要对数据进行归一化？
http://blog.csdn.net/xbmatrix/article/details/56695825
归一化化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。
1）归一化后加快了梯度下降求最优解的速度。等高线变得显得圆滑，在梯度下降进行求解时能较快的收敛。如果不做归一化，梯度下降过程容易走之字，很难收敛甚至不能收敛
2）把有量纲表达式变为无量纲表达式, 有可能提高精度。一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）
3) 逻辑回归等模型先验假设数据服从正态分布。
#### 17.	哪些机器学习算法不需要做归一化处理？
概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、gbdt、xgboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。
#### 18.	特征向量的归一化方法
线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)
对数函数转换，表达式如下：y=log10 (x)
反余切函数转换 ，表达式如下：y=arctan(x)*2/PI
减去均值，乘以方差：y=(x-means)/ 

#### 19.	标准化与归一化的区别
简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下：
 
#### 20.	特征向量的缺失值处理
1. 缺失值较多.直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。
2. 缺失值较少,其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:
1) 把NaN直接作为一个特征，假设用0表示；
2) 用均值填充；
3) 用随机森林等算法预测填充

#### 21.	随机森林如何处理缺失值（http://charleshm.github.io/2016/03/Random-Forest-Tricks/）
方法一（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。
方法二（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似12。

#### 22.	随机森林如何评估特征重要性（http://charleshm.github.io/2016/03/Random-Forest-Tricks/）
衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：
1) Decrease GINI： 对于回归问题，直接使用argmax(Var−VarLeft−VarRight)作为评判标准，即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight。
2) Decrease Accuracy：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要。
#### 23.	如何进行特征选择？
特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解
常见的特征选择方式：
1.	去除方差较小的特征
2.	正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
3.	随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
4.	稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。
#### 24.	深度学习 vs 机器学习 vs 模式识别
模式识别：智能程序的诞生。
机器学习：从样本中学习的智能程序。
深度学习：一统江湖的架构。受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络。
#### 25.	例举几个常用的python分析数据包及其作用
数据处理和分析：NumPy, SciPy, Pandas
机器学习：SciKit
可视化： Matplotlib, Seaborn
