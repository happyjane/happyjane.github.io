---
layout: post
title: "梯度提升方法-Gradient Boosting"
date: 2017-09-27 18:00:00 +0800 
categories: 研究生涯
tag: Algorithms
---
* content
{:toc}


这里收集梯度提升方法的自我理解及思考。

<!-- more -->

## 1. 算法介绍

>梯度提升是先根据初始模型计算伪残差，之后建立一个基学习器来解释伪残差，该基学习器是在梯度方向上减少残差。再将基学习器乘上权重系数(学习速率)和原来的模型进行线性组合形成新的模型。这样反复迭代就可以找到一个使损失函数的期望达到最小的模型。在训练基学习器时可以使用再抽样方法，此时就称之为随机梯度提升算法stochastic gradient boosting。


## 2. 算法推导

  待后补

## 3. 参数设置

>机器学习在使用过程中，很关键的一步便是调参，机器学习中主要参数包括以下几个：
>- 损失函数的形式(distribution):
>  损失函数的形式容易设定,分类问题一般选择bernoulli分布,而回归问题可以选择gaussian分布。
>- 迭代次数(n.trees):
>  n.trees参数一般建议设在3000-10000之间。
>- 学习速率(shrinkage)
>  学习速率方面，我们都知道步子迈得太大容易扯着，所以学习速率是越小越好，但是步子太小的话，步数就得增加，也就是训练的迭代次数需要加大才能使模型达到最优，这样训练所需时间和计算资源也相应加大了。作者的经验法则是设置shrinkage参数在0.01-0.001之间.
>- 再抽样比率(bag.fraction)
>- 决策树的深度(interaction.depth)

---

编辑备注：

+ 2017-09-27第一次编辑
